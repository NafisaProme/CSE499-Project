{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41f27d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.83\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Loading the dataset\n",
    "df = pd.read_csv('output.csv')\n",
    "\n",
    "# Split the dataset into features (X) and target (y)\n",
    "X = df.drop('Disease', axis=1)\n",
    "y = df['Disease']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the RandomForestClassifier with multiple Decision Trees (n_estimators)\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# checking which diseases are predicted wrong \n",
    "# for actual, predicted in zip(y_test, y_pred):\n",
    "#     if actual != predicted:\n",
    "#         print(f\"Actual: {actual}, Predicted: {predicted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83261d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.83\n"
     ]
    }
   ],
   "source": [
    "# Initialize the RandomForestClassifier with multiple Decision Trees (n_estimators)\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "print(f'Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abee8e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the 'clf' model to a pickle file\n",
    "with open('model.pkl', 'wb') as file:\n",
    "    pickle.dump(clf, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c598075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in y: {'Hypertension Pulmonary', 'Allergy', 'Kidney Failure Acute', 'Ketoacidosis Diabetic', 'Kidney Disease', 'Incontinence', 'Pancreatitis', 'Sepsis (Invertebrate)', 'Deep Vein Thrombosis', 'Thrombus', 'Hiv Infections', 'Pyelonephritis', 'Stenosis Aortic Valve', 'Hypertension ', 'Hyperglycemia', 'Influenza', 'Depressive Disorder', 'Neutropenia', 'Bronchial Asthma', 'Respiratory Failure', 'Acne', 'Gout', 'Tonic-Clonic Seizures', 'Psychotic Disorder', 'Carcinoma Of Lung', 'Deglutition Disorder', 'Primary Malignant Neoplasm', 'Adhesion', 'Fungal infection', 'Accidentcerebrovascular', 'Paralysis (brain hemorrhage)', 'Osteoarthristis', 'Parkinson Disease', 'Myocardial Infarction', 'Hepatitis C', 'Gastroenteritis', 'Adenocarcinoma', 'Pericardial Effusion Body Substance', 'Hypoglycemia', 'Peptic ulcer diseae', 'Drug Reaction', 'Carcinoma', 'Coronary Heart Disease', 'Personality Disorder', 'Schizophrenia', 'Heart attack', 'Hypothyroidism', 'Jaundice', 'Cellulitis', 'Suicide Attempt', 'Malaria', 'Obesity Morbid', 'Lymphatic Diseases', 'Edema Pulmonary', 'Confusion', \"Alzheimer'S Disease\", 'Benign Prostatic Hypertrophy', 'Infection Urinary Tract', 'Hepatitis D', 'Hyperbilirubinemia', 'Chronic Obstructive Airway Disease', 'Paroxysmaldyspnea', 'Diabetes ', 'Ulcer Peptic', 'Migraine Disorders', 'Tricuspid Valve Insufficiency', 'Hemorrhoids', 'Common Cold', 'Hepatitis', 'Endocarditis', 'Ileus', 'Osteomyelitis', 'Obesity', 'Encephalopathy', 'AIDS', 'Alcoholic hepatitis', 'Migraine', 'Diverticulitis', 'Degenerativepolyarthritis', 'Diabetes', 'Arthritis', 'Bipolar Disorder', 'Osteoporosis', 'Spasm Bronchial', 'Dependence', 'Carcinoma Breast', 'Varicose veins', 'Malignantneoplasms', 'Chronic cholestasis', 'Failure Heart Congestive', 'Transient Ischemic Attack', 'Hypercholesterolemia', 'Dehydration', 'Sickle Cell Anemia', 'Urinary tract infection', 'Bacteremia', 'Overload Fluid', 'Mitral Valve Insufficiency', 'Hyperlipidemia', 'Decubitus Ulcer', 'Carcinoma Colon', 'Chicken pox', 'Anemia', 'Typhoid', 'Asthma', 'Manic Disorder', 'Aphasia', 'Gastritis', 'Pneumonia Aspiration', '(vertigo) Paroymsal  Positional Vertigo', 'Primary Carcinoma Of The Liver Cells', 'Pneumothorax', 'Dengue', 'Epilepsy', 'Anxiety State', 'Exanthema', 'Ischemia', 'Hemiparesis', 'Upper Respiratory Infection', 'Gastroesophageal Reflux Disease', 'Failure Heart', 'Delirium', 'Paranoia', 'Dimorphic hemmorhoids(piles)', 'Dementia', 'Cirrhosis', 'Carcinoma Prostate', 'Cholecystitis', 'Cardiomyopathy', 'Hypertensive Disease', 'Embolism Pulmonary', 'Lymphoma', 'Biliary Calculus', 'Chronic Kidney Failure', 'hepatitis A', 'Glaucoma', 'Peripheral Vascular Disease', 'Neuropathy', 'Thrombocytopaenia', 'Hepatitis B', 'Emphysema Pulmonary', 'Colitis', 'Impetigo', 'Tachycardia Sinus', 'Chronic Alcoholic Intoxication', 'Hernia', 'Hyperthyroidism', 'Oralcandidiasis', 'Neoplasm', 'Diverticulosis', 'Tuberculosis', 'Fibroid Tumor', 'Psoriasis', 'Delusion', 'Insufficiency Renal', 'Herniahiatal', 'Failure Kidney', 'Affect Labile', 'Cervical spondylosis', 'Pneumonia', 'Neoplasm Metastasis', 'Pancytopenia', 'GERD', 'Pneumocystiscariniipneumonia', 'Infection', 'Melanoma', 'Hepatitis E', 'Bronchitis'}\n",
      "Unique values in y_encoded: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Assuming your target variable is 'y'\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Print unique values before and after encoding\n",
    "print(\"Unique values in y:\", set(y))\n",
    "print(\"Unique values in y_encoded:\", set(y_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7a7f763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 96.83\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Create an SVM classifier\n",
    "model = make_pipeline(StandardScaler(), SVC())\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "print(f\"SVM Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c0356e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy: 96.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Create a KNN classifier\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "print(f\"KNN Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5056d332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 96.83\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Create a Naive Bayes classifier\n",
    "model = GaussianNB()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "print(f\"Naive Bayes Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00e12c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy: 0.1671612265084075\n",
      "SVM Accuracy: 0.9683481701285855\n",
      "KNN Accuracy: 0.9683481701285855\n",
      "Voting Classifier Accuracy: 0.9683481701285855\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# AdaBoost Classifier\n",
    "base_classifier = DecisionTreeClassifier(max_depth=1)  # You can use any classifier as the base learner\n",
    "adaboost_classifier = AdaBoostClassifier(base_classifier, n_estimators=50, learning_rate=1.0, random_state=42)\n",
    "\n",
    "# Support Vector Machine (SVM) Classifier\n",
    "svm_classifier = SVC(probability=True, kernel='linear', C=1.0)\n",
    "\n",
    "# K-Nearest Neighbors (KNN) Classifier\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Voting Classifier\n",
    "voting_classifier = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('adaboost', adaboost_classifier),\n",
    "        ('svm', svm_classifier),\n",
    "        ('knn', knn_classifier)\n",
    "    ],\n",
    "    voting='soft'  # 'hard' for majority voting, 'soft' for weighted voting based on probabilities\n",
    ")\n",
    "\n",
    "# Train the classifiers\n",
    "adaboost_classifier.fit(X_train, y_train)\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "voting_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "adaboost_pred = adaboost_classifier.predict(X_test)\n",
    "svm_pred = svm_classifier.predict(X_test)\n",
    "knn_pred = knn_classifier.predict(X_test)\n",
    "voting_pred = voting_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the performance\n",
    "print(f\"AdaBoost Accuracy: {accuracy_score(y_test, adaboost_pred)}\")\n",
    "print(f\"SVM Accuracy: {accuracy_score(y_test, svm_pred)}\")\n",
    "print(f\"KNN Accuracy: {accuracy_score(y_test, knn_pred)}\")\n",
    "print(f\"Voting Classifier Accuracy: {accuracy_score(y_test, voting_pred)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f25c8914",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.09792284866468842\n",
      "SVM Accuracy: 0.9683481701285855\n",
      "KNN Accuracy: 0.9683481701285855\n",
      "Random Forest Accuracy: 0.9683481701285855\n",
      "Voting Classifier Accuracy: 0.9683481701285855\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Decision Tree Classifier\n",
    "base_classifier_dt = DecisionTreeClassifier(max_depth=5)\n",
    "\n",
    "# Support Vector Machine (SVM) Classifier\n",
    "base_classifier_svm = SVC(probability=True, kernel='linear', C=1.0)\n",
    "\n",
    "# K-Nearest Neighbors (KNN) Classifier\n",
    "base_classifier_knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Random Forest Classifier\n",
    "base_classifier_rf = RandomForestClassifier(n_estimators=150, random_state=42)\n",
    "\n",
    "# Voting Classifier\n",
    "voting_classifier = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('decision_tree', base_classifier_dt),\n",
    "        ('svm', base_classifier_svm),\n",
    "        ('knn', base_classifier_knn),\n",
    "        ('random_forest', base_classifier_rf)\n",
    "    ],\n",
    "    voting='soft'  # 'hard' for majority voting, 'soft' for weighted voting based on probabilities\n",
    ")\n",
    "\n",
    "# Train the classifiers\n",
    "base_classifier_dt.fit(X_train, y_train)\n",
    "base_classifier_svm.fit(X_train, y_train)\n",
    "base_classifier_knn.fit(X_train, y_train)\n",
    "base_classifier_rf.fit(X_train, y_train)\n",
    "voting_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "dt_pred = base_classifier_dt.predict(X_test)\n",
    "svm_pred = base_classifier_svm.predict(X_test)\n",
    "knn_pred = base_classifier_knn.predict(X_test)\n",
    "rf_pred = base_classifier_rf.predict(X_test)\n",
    "voting_pred = voting_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the performance\n",
    "print(f\"Decision Tree Accuracy: {accuracy_score(y_test, dt_pred)}\")\n",
    "print(f\"SVM Accuracy: {accuracy_score(y_test, svm_pred)}\")\n",
    "print(f\"KNN Accuracy: {accuracy_score(y_test, knn_pred)}\")\n",
    "print(f\"Random Forest Accuracy: {accuracy_score(y_test, rf_pred)}\")\n",
    "print(f\"Voting Classifier Accuracy: {accuracy_score(y_test, voting_pred)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b8a5498",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.9683481701285855\n",
      "Gradient Boosting Accuracy: 0.1493570722057369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.09792284866468842\n",
      "SVM Accuracy: 0.9683481701285855\n",
      "KNN Accuracy: 0.9683481701285855\n",
      "Random Forest Accuracy: 0.9683481701285855\n",
      "AdaBoost Accuracy: 0.1671612265084075\n",
      "Voting Classifier Accuracy: 0.9683481701285855\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Decision Tree Classifier\n",
    "base_classifier_dt = DecisionTreeClassifier(max_depth=5)\n",
    "base_classifier_dt.fit(X_train, y_train)\n",
    "dt_pred = base_classifier_dt.predict(X_test)\n",
    "dt_accuracy = accuracy_score(y_test, dt_pred)\n",
    "\n",
    "# Support Vector Machine (SVM) Classifier\n",
    "base_classifier_svm = SVC(probability=True, kernel='linear', C=1.0)\n",
    "base_classifier_svm.fit(X_train, y_train)\n",
    "svm_pred = base_classifier_svm.predict(X_test)\n",
    "svm_accuracy = accuracy_score(y_test, svm_pred)\n",
    "# RandomForest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "rf_pred = rf_classifier.predict(X_test)\n",
    "print(f\"Random Forest Accuracy: {accuracy_score(y_test, rf_pred)}\")\n",
    "\n",
    "# GradientBoosting Classifier\n",
    "gb_classifier = GradientBoostingClassifier(n_estimators=20, learning_rate=1.0, max_depth=1, random_state=42)\n",
    "gb_classifier.fit(X_train, y_train)\n",
    "gb_pred = gb_classifier.predict(X_test)\n",
    "print(f\"Gradient Boosting Accuracy: {accuracy_score(y_test, gb_pred)}\")\n",
    "\n",
    "# SVM Classifier\n",
    "svm_classifier = SVC(probability=True, kernel='linear', C=1.0)\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "svm_pred = svm_classifier.predict(X_test)\n",
    "svm_accuracy = accuracy_score(y_test, svm_pred)\n",
    "\n",
    "# K-Nearest Neighbors (KNN) Classifier\n",
    "base_classifier_knn = KNeighborsClassifier(n_neighbors=5)\n",
    "base_classifier_knn.fit(X_train, y_train)\n",
    "knn_pred = base_classifier_knn.predict(X_test)\n",
    "knn_accuracy = accuracy_score(y_test, knn_pred)\n",
    "\n",
    "# Random Forest Classifier\n",
    "base_classifier_rf = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "base_classifier_rf.fit(X_train, y_train)\n",
    "rf_pred = base_classifier_rf.predict(X_test)\n",
    "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
    "\n",
    "# AdaBoost Classifier\n",
    "base_classifier_ada = AdaBoostClassifier(n_estimators=50, learning_rate=1)\n",
    "base_classifier_ada.fit(X_train, y_train)\n",
    "ada_pred = base_classifier_ada.predict(X_test)\n",
    "ada_accuracy = accuracy_score(y_test, ada_pred)\n",
    "\n",
    "# Voting Classifier\n",
    "voting_classifier = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('decision_tree', base_classifier_dt),\n",
    "        ('svm', base_classifier_svm),\n",
    "        ('knn', base_classifier_knn),\n",
    "        ('random_forest', base_classifier_rf),\n",
    "        ('ada_boost', base_classifier_ada)\n",
    "    ],\n",
    "    voting='soft'  # 'hard' for majority voting, 'soft' for weighted voting based on probabilities\n",
    ")\n",
    "\n",
    "voting_classifier.fit(X_train, y_train)\n",
    "voting_pred = voting_classifier.predict(X_test)\n",
    "voting_accuracy = accuracy_score(y_test, voting_pred)\n",
    "\n",
    "# Print accuracies together\n",
    "print(f\"Decision Tree Accuracy: {dt_accuracy}\")\n",
    "print(f\"SVM Accuracy: {svm_accuracy}\")\n",
    "print(f\"KNN Accuracy: {knn_accuracy}\")\n",
    "print(f\"Random Forest Accuracy: {rf_accuracy}\")\n",
    "print(f\"AdaBoost Accuracy: {ada_accuracy}\")\n",
    "print(f\"Voting Classifier Accuracy: {voting_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e49ee86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier Accuracy: 0.9683481701285855\n",
      "Voting Classifier Precision: 0.9516013034728258\n",
      "Voting Classifier Recall: 0.9683481701285855\n",
      "Voting Classifier F1 Score: 0.9583380624599073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "voting_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Voting Classifier Accuracy: {voting_accuracy}\")\n",
    "\n",
    "voting_precision = precision_score(y_test, y_pred, average='weighted')\n",
    "voting_recall = recall_score(y_test, y_pred, average='weighted')\n",
    "voting_f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Voting Classifier Precision: {voting_precision}\")\n",
    "print(f\"Voting Classifier Recall: {voting_recall}\")\n",
    "print(f\"Voting Classifier F1 Score: {voting_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4ee17add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Accuracy: 97.42828882294758\n",
      "Precision: 0.9710991592482691\n",
      "Recall: 0.9742828882294757\n",
      "F1 Score: 0.972502706725819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "df = pd.read_csv('output.csv')\n",
    "\n",
    "# Split the dataset into features (X) and target (y)\n",
    "X = df.drop('Disease', axis=1)\n",
    "y = df['Disease']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=32)\n",
    "\n",
    "# Initialize the RandomForestClassifier with multiple Decision Trees (n_estimators)\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f'Decision Tree')\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8decdd60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest:\n",
      "Accuracy: 97.42828882294758\n",
      "Precision: 0.9981536432575008\n",
      "Recall: 0.9742828882294757\n",
      "F1 Score: 0.973810934571359\n",
      "\n",
      "SVM:\n",
      "Accuracy: 97.42828882294758\n",
      "Precision: 0.9930678362923665\n",
      "Recall: 0.9742828882294757\n",
      "F1 Score: 0.9707203984680051\n",
      "\n",
      "Voting Classifier:\n",
      "Accuracy: 97.42828882294758\n",
      "Precision: 1.0\n",
      "Recall: 0.9742828882294757\n",
      "F1 Score: 0.9747659466795482\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "df = pd.read_csv('output.csv')\n",
    "\n",
    "# # Split the dataset into features (X) and target (y)\n",
    "# X = df.drop('Disease', axis=1)\n",
    "# y = df['Disease']\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=35)\n",
    "\n",
    "# Decision Tree Classifier\n",
    "base_classifier_dt = DecisionTreeClassifier()\n",
    "base_classifier_dt.fit(X_train, y_train)\n",
    "dt_pred = base_classifier_dt.predict(X_test)\n",
    "dt_accuracy = accuracy_score(y_test, dt_pred)\n",
    "dt_precision = precision_score(y_test, dt_pred, average='weighted', zero_division=1)\n",
    "dt_recall = recall_score(y_test, dt_pred, average='weighted', zero_division=1)\n",
    "dt_f1 = f1_score(y_test, dt_pred, average='weighted', zero_division=1)\n",
    "\n",
    "# Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "rf_pred = rf_classifier.predict(X_test)\n",
    "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
    "rf_precision = precision_score(y_test, rf_pred, average='weighted', zero_division=1)\n",
    "rf_recall = recall_score(y_test, rf_pred, average='weighted', zero_division=1)\n",
    "rf_f1 = f1_score(y_test, rf_pred, average='weighted', zero_division=1)\n",
    "\n",
    "# Support Vector Machine (SVM) Classifier\n",
    "base_classifier_svm = SVC(probability=True, kernel='linear', C=1.0)\n",
    "base_classifier_svm.fit(X_train, y_train)\n",
    "svm_pred = base_classifier_svm.predict(X_test)\n",
    "svm_accuracy = accuracy_score(y_test, svm_pred)\n",
    "svm_precision = precision_score(y_test, svm_pred, average='weighted', zero_division=1)\n",
    "svm_recall = recall_score(y_test, svm_pred, average='weighted', zero_division=1)\n",
    "svm_f1 = f1_score(y_test, svm_pred, average='weighted', zero_division=1)\n",
    "\n",
    "# Voting Classifier\n",
    "voting_classifier = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('decision_tree', clf),\n",
    "        ('svm', base_classifier_svm),\n",
    "        ('random_forest', rf_classifier),\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "voting_classifier.fit(X_train, y_train)\n",
    "voting_pred = voting_classifier.predict(X_test)\n",
    "voting_accuracy = accuracy_score(y_test, voting_pred)\n",
    "voting_precision = precision_score(y_test, voting_pred, average='weighted', zero_division=1)\n",
    "voting_recall = recall_score(y_test, voting_pred, average='weighted', zero_division=1)\n",
    "voting_f1 = f1_score(y_test, voting_pred, average='weighted', zero_division=1)\n",
    "\n",
    "print(\"\\nRandom Forest:\")\n",
    "print(f\"Accuracy: {rf_accuracy * 100}\")\n",
    "print(f\"Precision: {rf_precision}\")\n",
    "print(f\"Recall: {rf_recall}\")\n",
    "print(f\"F1 Score: {rf_f1}\")\n",
    "\n",
    "print(\"\\nSVM:\")\n",
    "print(f\"Accuracy: {svm_accuracy * 100}\")\n",
    "print(f\"Precision: {svm_precision}\")\n",
    "print(f\"Recall: {svm_recall}\")\n",
    "print(f\"F1 Score: {svm_f1}\")\n",
    "\n",
    "print(\"\\nVoting Classifier:\")\n",
    "print(f\"Accuracy: {voting_accuracy * 100}\")\n",
    "print(f\"Precision: {voting_precision}\")\n",
    "print(f\"Recall: {voting_recall}\")\n",
    "print(f\"F1 Score: {voting_f1}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
