{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41f27d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.83\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Loading the dataset\n",
    "df = pd.read_csv('output.csv')\n",
    "\n",
    "# Split the dataset into features (X) and target (y)\n",
    "X = df.drop('Disease', axis=1)\n",
    "y = df['Disease']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the RandomForestClassifier with multiple Decision Trees (n_estimators)\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# checking which diseases are predicted wrong \n",
    "# for actual, predicted in zip(y_test, y_pred):\n",
    "#     if actual != predicted:\n",
    "#         print(f\"Actual: {actual}, Predicted: {predicted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83261d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.83\n"
     ]
    }
   ],
   "source": [
    "# Initialize the RandomForestClassifier with multiple Decision Trees (n_estimators)\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "print(f'Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abee8e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the 'clf' model to a pickle file\n",
    "with open('model.pkl', 'wb') as file:\n",
    "    pickle.dump(clf, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c598075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in y: {'Obesity Morbid', \"Alzheimer'S Disease\", 'Hiv Infections', 'Thrombus', 'Hypertensive Disease', 'Edema Pulmonary', 'Psoriasis', 'Sepsis (Invertebrate)', 'AIDS', 'Benign Prostatic Hypertrophy', 'Stenosis Aortic Valve', 'Alcoholic hepatitis', 'Hyperbilirubinemia', 'Gastroenteritis', 'Infection Urinary Tract', 'Carcinoma Breast', 'Ulcer Peptic', 'Pericardial Effusion Body Substance', 'Cellulitis', 'Ketoacidosis Diabetic', 'Spasm Bronchial', 'Dependence', 'Schizophrenia', 'Jaundice', 'Cardiomyopathy', 'Transient Ischemic Attack', 'Tricuspid Valve Insufficiency', 'Impetigo', 'Peripheral Vascular Disease', 'Paralysis (brain hemorrhage)', 'Hepatitis E', 'Dehydration', 'Kidney Failure Acute', 'Tuberculosis', 'Arthritis', 'Cirrhosis', 'Osteoporosis', 'Suicide Attempt', 'Delusion', 'Fungal infection', 'Decubitus Ulcer', 'Dimorphic hemmorhoids(piles)', 'Hypercholesterolemia', 'Bronchitis', 'Malignantneoplasms', 'Hepatitis', 'Obesity', 'Heart attack', 'Psychotic Disorder', 'Kidney Disease', 'Gastritis', 'Neoplasm Metastasis', 'Glaucoma', 'Urinary tract infection', 'Hyperthyroidism', 'Bipolar Disorder', 'Malaria', 'Myocardial Infarction', 'Mitral Valve Insufficiency', 'Migraine Disorders', 'Exanthema', 'Pneumonia Aspiration', 'Primary Malignant Neoplasm', 'Carcinoma Colon', 'Asthma', 'Adenocarcinoma', 'Pneumocystiscariniipneumonia', 'Melanoma', 'Pancytopenia', 'Herniahiatal', 'Drug Reaction', 'Failure Kidney', 'Pyelonephritis', 'Dementia', 'Diverticulosis', 'Insufficiency Renal', 'Hepatitis D', 'hepatitis A', 'Varicose veins', 'Lymphatic Diseases', 'Hyperglycemia', 'Thrombocytopaenia', 'Parkinson Disease', 'Tonic-Clonic Seizures', 'Epilepsy', 'Diabetes ', 'Hypothyroidism', 'Typhoid', 'Depressive Disorder', 'Accidentcerebrovascular', 'Carcinoma', 'Personality Disorder', 'Encephalopathy', 'Tachycardia Sinus', 'Deep Vein Thrombosis', 'Adhesion', 'Chronic Alcoholic Intoxication', 'Ileus', 'Hypoglycemia', 'Colitis', 'Ischemia', 'Coronary Heart Disease', 'Chronic Kidney Failure', 'Gout', 'Lymphoma', 'Cervical spondylosis', 'Hyperlipidemia', 'Peptic ulcer diseae', 'Osteoarthristis', 'Paroxysmaldyspnea', 'Diverticulitis', 'Neutropenia', 'Failure Heart', 'Hepatitis B', 'Influenza', 'Chicken pox', '(vertigo) Paroymsal  Positional Vertigo', 'Paranoia', 'Pneumothorax', 'Hypertension ', 'Bronchial Asthma', 'Overload Fluid', 'Hernia', 'Migraine', 'GERD', 'Hypertension Pulmonary', 'Hemorrhoids', 'Common Cold', 'Allergy', 'Primary Carcinoma Of The Liver Cells', 'Upper Respiratory Infection', 'Affect Labile', 'Neuropathy', 'Chronic cholestasis', 'Carcinoma Prostate', 'Acne', 'Cholecystitis', 'Respiratory Failure', 'Degenerativepolyarthritis', 'Infection', 'Incontinence', 'Pancreatitis', 'Gastroesophageal Reflux Disease', 'Failure Heart Congestive', 'Biliary Calculus', 'Anxiety State', 'Delirium', 'Fibroid Tumor', 'Confusion', 'Bacteremia', 'Emphysema Pulmonary', 'Anemia', 'Hemiparesis', 'Osteomyelitis', 'Dengue', 'Aphasia', 'Chronic Obstructive Airway Disease', 'Hepatitis C', 'Manic Disorder', 'Pneumonia', 'Neoplasm', 'Diabetes', 'Deglutition Disorder', 'Embolism Pulmonary', 'Oralcandidiasis', 'Endocarditis', 'Carcinoma Of Lung', 'Sickle Cell Anemia'}\n",
      "Unique values in y_encoded: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Assuming your target variable is 'y'\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Print unique values before and after encoding\n",
    "print(\"Unique values in y:\", set(y))\n",
    "print(\"Unique values in y_encoded:\", set(y_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7a7f763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 96.83\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Create an SVM classifier\n",
    "model = make_pipeline(StandardScaler(), SVC())\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "print(f\"SVM Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c0356e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy: 96.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Create a KNN classifier\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "print(f\"KNN Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5056d332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 96.83\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Create a Naive Bayes classifier\n",
    "model = GaussianNB()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "print(f\"Naive Bayes Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00e12c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy: 0.1671612265084075\n",
      "SVM Accuracy: 0.9683481701285855\n",
      "KNN Accuracy: 0.9683481701285855\n",
      "Voting Classifier Accuracy: 0.9683481701285855\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# AdaBoost Classifier\n",
    "base_classifier = DecisionTreeClassifier(max_depth=1)  # You can use any classifier as the base learner\n",
    "adaboost_classifier = AdaBoostClassifier(base_classifier, n_estimators=50, learning_rate=1.0, random_state=42)\n",
    "\n",
    "# Support Vector Machine (SVM) Classifier\n",
    "svm_classifier = SVC(probability=True, kernel='linear', C=1.0)\n",
    "\n",
    "# K-Nearest Neighbors (KNN) Classifier\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Voting Classifier\n",
    "voting_classifier = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('adaboost', adaboost_classifier),\n",
    "        ('svm', svm_classifier),\n",
    "        ('knn', knn_classifier)\n",
    "    ],\n",
    "    voting='soft'  # 'hard' for majority voting, 'soft' for weighted voting based on probabilities\n",
    ")\n",
    "\n",
    "# Train the classifiers\n",
    "adaboost_classifier.fit(X_train, y_train)\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "voting_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "adaboost_pred = adaboost_classifier.predict(X_test)\n",
    "svm_pred = svm_classifier.predict(X_test)\n",
    "knn_pred = knn_classifier.predict(X_test)\n",
    "voting_pred = voting_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the performance\n",
    "print(f\"AdaBoost Accuracy: {accuracy_score(y_test, adaboost_pred)}\")\n",
    "print(f\"SVM Accuracy: {accuracy_score(y_test, svm_pred)}\")\n",
    "print(f\"KNN Accuracy: {accuracy_score(y_test, knn_pred)}\")\n",
    "print(f\"Voting Classifier Accuracy: {accuracy_score(y_test, voting_pred)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f25c8914",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.09792284866468842\n",
      "SVM Accuracy: 0.9683481701285855\n",
      "KNN Accuracy: 0.9683481701285855\n",
      "Random Forest Accuracy: 0.9683481701285855\n",
      "Voting Classifier Accuracy: 0.9683481701285855\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Decision Tree Classifier\n",
    "base_classifier_dt = DecisionTreeClassifier(max_depth=5)\n",
    "\n",
    "# Support Vector Machine (SVM) Classifier\n",
    "base_classifier_svm = SVC(probability=True, kernel='linear', C=1.0)\n",
    "\n",
    "# K-Nearest Neighbors (KNN) Classifier\n",
    "base_classifier_knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Random Forest Classifier\n",
    "base_classifier_rf = RandomForestClassifier(n_estimators=150, random_state=42)\n",
    "\n",
    "# Voting Classifier\n",
    "voting_classifier = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('decision_tree', base_classifier_dt),\n",
    "        ('svm', base_classifier_svm),\n",
    "        ('knn', base_classifier_knn),\n",
    "        ('random_forest', base_classifier_rf)\n",
    "    ],\n",
    "    voting='soft'  # 'hard' for majority voting, 'soft' for weighted voting based on probabilities\n",
    ")\n",
    "\n",
    "# Train the classifiers\n",
    "base_classifier_dt.fit(X_train, y_train)\n",
    "base_classifier_svm.fit(X_train, y_train)\n",
    "base_classifier_knn.fit(X_train, y_train)\n",
    "base_classifier_rf.fit(X_train, y_train)\n",
    "voting_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "dt_pred = base_classifier_dt.predict(X_test)\n",
    "svm_pred = base_classifier_svm.predict(X_test)\n",
    "knn_pred = base_classifier_knn.predict(X_test)\n",
    "rf_pred = base_classifier_rf.predict(X_test)\n",
    "voting_pred = voting_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the performance\n",
    "print(f\"Decision Tree Accuracy: {accuracy_score(y_test, dt_pred)}\")\n",
    "print(f\"SVM Accuracy: {accuracy_score(y_test, svm_pred)}\")\n",
    "print(f\"KNN Accuracy: {accuracy_score(y_test, knn_pred)}\")\n",
    "print(f\"Random Forest Accuracy: {accuracy_score(y_test, rf_pred)}\")\n",
    "print(f\"Voting Classifier Accuracy: {accuracy_score(y_test, voting_pred)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b8a5498",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.9683481701285855\n",
      "Gradient Boosting Accuracy: 0.1493570722057369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.09792284866468842\n",
      "SVM Accuracy: 0.9683481701285855\n",
      "KNN Accuracy: 0.9683481701285855\n",
      "Random Forest Accuracy: 0.9683481701285855\n",
      "AdaBoost Accuracy: 0.1671612265084075\n",
      "Voting Classifier Accuracy: 0.9683481701285855\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Decision Tree Classifier\n",
    "base_classifier_dt = DecisionTreeClassifier(max_depth=5)\n",
    "base_classifier_dt.fit(X_train, y_train)\n",
    "dt_pred = base_classifier_dt.predict(X_test)\n",
    "dt_accuracy = accuracy_score(y_test, dt_pred)\n",
    "\n",
    "# Support Vector Machine (SVM) Classifier\n",
    "base_classifier_svm = SVC(probability=True, kernel='linear', C=1.0)\n",
    "base_classifier_svm.fit(X_train, y_train)\n",
    "svm_pred = base_classifier_svm.predict(X_test)\n",
    "svm_accuracy = accuracy_score(y_test, svm_pred)\n",
    "# RandomForest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "rf_pred = rf_classifier.predict(X_test)\n",
    "print(f\"Random Forest Accuracy: {accuracy_score(y_test, rf_pred)}\")\n",
    "\n",
    "# GradientBoosting Classifier\n",
    "gb_classifier = GradientBoostingClassifier(n_estimators=20, learning_rate=1.0, max_depth=1, random_state=42)\n",
    "gb_classifier.fit(X_train, y_train)\n",
    "gb_pred = gb_classifier.predict(X_test)\n",
    "print(f\"Gradient Boosting Accuracy: {accuracy_score(y_test, gb_pred)}\")\n",
    "\n",
    "# SVM Classifier\n",
    "svm_classifier = SVC(probability=True, kernel='linear', C=1.0)\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "svm_pred = svm_classifier.predict(X_test)\n",
    "svm_accuracy = accuracy_score(y_test, svm_pred)\n",
    "\n",
    "# K-Nearest Neighbors (KNN) Classifier\n",
    "base_classifier_knn = KNeighborsClassifier(n_neighbors=5)\n",
    "base_classifier_knn.fit(X_train, y_train)\n",
    "knn_pred = base_classifier_knn.predict(X_test)\n",
    "knn_accuracy = accuracy_score(y_test, knn_pred)\n",
    "\n",
    "# Random Forest Classifier\n",
    "base_classifier_rf = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "base_classifier_rf.fit(X_train, y_train)\n",
    "rf_pred = base_classifier_rf.predict(X_test)\n",
    "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
    "\n",
    "# AdaBoost Classifier\n",
    "base_classifier_ada = AdaBoostClassifier(n_estimators=50, learning_rate=1)\n",
    "base_classifier_ada.fit(X_train, y_train)\n",
    "ada_pred = base_classifier_ada.predict(X_test)\n",
    "ada_accuracy = accuracy_score(y_test, ada_pred)\n",
    "\n",
    "# Voting Classifier\n",
    "voting_classifier = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('decision_tree', base_classifier_dt),\n",
    "        ('svm', base_classifier_svm),\n",
    "        ('knn', base_classifier_knn),\n",
    "        ('random_forest', base_classifier_rf),\n",
    "        ('ada_boost', base_classifier_ada)\n",
    "    ],\n",
    "    voting='soft'  # 'hard' for majority voting, 'soft' for weighted voting based on probabilities\n",
    ")\n",
    "\n",
    "voting_classifier.fit(X_train, y_train)\n",
    "voting_pred = voting_classifier.predict(X_test)\n",
    "voting_accuracy = accuracy_score(y_test, voting_pred)\n",
    "\n",
    "# Print accuracies together\n",
    "print(f\"Decision Tree Accuracy: {dt_accuracy}\")\n",
    "print(f\"SVM Accuracy: {svm_accuracy}\")\n",
    "print(f\"KNN Accuracy: {knn_accuracy}\")\n",
    "print(f\"Random Forest Accuracy: {rf_accuracy}\")\n",
    "print(f\"AdaBoost Accuracy: {ada_accuracy}\")\n",
    "print(f\"Voting Classifier Accuracy: {voting_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e49ee86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier Accuracy: 0.9683481701285855\n",
      "Voting Classifier Precision: 0.9516013034728258\n",
      "Voting Classifier Recall: 0.9683481701285855\n",
      "Voting Classifier F1 Score: 0.9583380624599073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "voting_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Voting Classifier Accuracy: {voting_accuracy}\")\n",
    "\n",
    "voting_precision = precision_score(y_test, y_pred, average='weighted')\n",
    "voting_recall = recall_score(y_test, y_pred, average='weighted')\n",
    "voting_f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Voting Classifier Precision: {voting_precision}\")\n",
    "print(f\"Voting Classifier Recall: {voting_recall}\")\n",
    "print(f\"Voting Classifier F1 Score: {voting_f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
