{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41f27d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.83\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Loading the dataset\n",
    "df = pd.read_csv('output.csv')\n",
    "\n",
    "# Split the dataset into features (X) and target (y)\n",
    "X = df.drop('Disease', axis=1)\n",
    "y = df['Disease']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the RandomForestClassifier with multiple Decision Trees (n_estimators)\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# checking which diseases are predicted wrong \n",
    "# for actual, predicted in zip(y_test, y_pred):\n",
    "#     if actual != predicted:\n",
    "#         print(f\"Actual: {actual}, Predicted: {predicted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83261d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.83\n"
     ]
    }
   ],
   "source": [
    "# Initialize the RandomForestClassifier with multiple Decision Trees (n_estimators)\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "print(f'Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abee8e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the 'clf' model to a pickle file\n",
    "with open('model.pkl', 'wb') as file:\n",
    "    pickle.dump(clf, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c598075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in y: {'Depressive Disorder', 'Ileus', 'Dementia', 'Typhoid', 'Carcinoma', 'Thrombus', 'Myocardial Infarction', 'Affect Labile', 'Paroxysmaldyspnea', 'Acne', 'Carcinoma Of Lung', 'Hemorrhoids', 'Hypertension Pulmonary', 'Tachycardia Sinus', 'Infection', 'Mitral Valve Insufficiency', 'Stenosis Aortic Valve', 'Osteoarthristis', 'Carcinoma Colon', 'Chronic Kidney Failure', 'Benign Prostatic Hypertrophy', 'Paranoia', 'Delirium', 'Hypercholesterolemia', 'hepatitis A', 'Chronic Alcoholic Intoxication', 'Emphysema Pulmonary', 'Carcinoma Prostate', 'Endocarditis', 'Colitis', 'Hemiparesis', 'Chronic cholestasis', 'Thrombocytopaenia', 'Hepatitis D', 'Gout', 'Encephalopathy', 'Anemia', 'Bacteremia', 'Deep Vein Thrombosis', 'Dehydration', 'Diabetes', 'Kidney Failure Acute', 'Infection Urinary Tract', 'Neuropathy', 'Chronic Obstructive Airway Disease', 'Obesity', 'Diabetes ', 'Adenocarcinoma', 'Hyperbilirubinemia', 'Hypertension ', 'Hepatitis', 'Gastroenteritis', 'Arthritis', 'Peripheral Vascular Disease', 'Gastritis', 'Epilepsy', 'Dependence', 'Tuberculosis', 'Hyperthyroidism', \"Alzheimer'S Disease\", 'Neutropenia', 'Accidentcerebrovascular', 'Confusion', 'Cellulitis', 'Urinary tract infection', '(vertigo) Paroymsal  Positional Vertigo', 'Hypertensive Disease', 'Upper Respiratory Infection', 'Delusion', 'Anxiety State', 'Ulcer Peptic', 'Degenerativepolyarthritis', 'Hiv Infections', 'Neoplasm Metastasis', 'Adhesion', 'Spasm Bronchial', 'Cirrhosis', 'GERD', 'Migraine Disorders', 'Hypoglycemia', 'Glaucoma', 'Cervical spondylosis', 'Bronchial Asthma', 'Alcoholic hepatitis', 'Failure Heart Congestive', 'Deglutition Disorder', 'Decubitus Ulcer', 'Gastroesophageal Reflux Disease', 'AIDS', 'Insufficiency Renal', 'Hernia', 'Hepatitis E', 'Sickle Cell Anemia', 'Pancytopenia', 'Jaundice', 'Parkinson Disease', 'Oralcandidiasis', 'Pneumonia', 'Exanthema', 'Lymphoma', 'Heart attack', 'Paralysis (brain hemorrhage)', 'Psoriasis', 'Common Cold', 'Pancreatitis', 'Peptic ulcer diseae', 'Melanoma', 'Lymphatic Diseases', 'Pneumonia Aspiration', 'Cholecystitis', 'Fibroid Tumor', 'Malaria', 'Allergy', 'Psychotic Disorder', 'Obesity Morbid', 'Bronchitis', 'Coronary Heart Disease', 'Hepatitis B', 'Pneumothorax', 'Sepsis (Invertebrate)', 'Hyperlipidemia', 'Personality Disorder', 'Suicide Attempt', 'Pneumocystiscariniipneumonia', 'Schizophrenia', 'Chicken pox', 'Overload Fluid', 'Primary Malignant Neoplasm', 'Osteoporosis', 'Hyperglycemia', 'Pericardial Effusion Body Substance', 'Asthma', 'Herniahiatal', 'Varicose veins', 'Biliary Calculus', 'Carcinoma Breast', 'Malignantneoplasms', 'Failure Kidney', 'Diverticulosis', 'Ketoacidosis Diabetic', 'Dimorphic hemmorhoids(piles)', 'Influenza', 'Transient Ischemic Attack', 'Hepatitis C', 'Hypothyroidism', 'Drug Reaction', 'Diverticulitis', 'Impetigo', 'Ischemia', 'Kidney Disease', 'Tonic-Clonic Seizures', 'Incontinence', 'Primary Carcinoma Of The Liver Cells', 'Aphasia', 'Neoplasm', 'Osteomyelitis', 'Pyelonephritis', 'Bipolar Disorder', 'Failure Heart', 'Migraine', 'Cardiomyopathy', 'Edema Pulmonary', 'Fungal infection', 'Respiratory Failure', 'Manic Disorder', 'Dengue', 'Tricuspid Valve Insufficiency', 'Embolism Pulmonary'}\n",
      "Unique values in y_encoded: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Assuming your target variable is 'y'\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Print unique values before and after encoding\n",
    "print(\"Unique values in y:\", set(y))\n",
    "print(\"Unique values in y_encoded:\", set(y_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7a7f763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 96.83\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Create an SVM classifier\n",
    "model = make_pipeline(StandardScaler(), SVC())\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "print(f\"SVM Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c0356e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy: 96.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Create a KNN classifier\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "print(f\"KNN Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5056d332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 96.83\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Create a Naive Bayes classifier\n",
    "model = GaussianNB()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "print(f\"Naive Bayes Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00e12c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy: 0.1671612265084075\n",
      "SVM Accuracy: 0.9683481701285855\n",
      "KNN Accuracy: 0.9683481701285855\n",
      "Voting Classifier Accuracy: 0.9683481701285855\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# AdaBoost Classifier\n",
    "base_classifier = DecisionTreeClassifier(max_depth=1)  # You can use any classifier as the base learner\n",
    "adaboost_classifier = AdaBoostClassifier(base_classifier, n_estimators=50, learning_rate=1.0, random_state=42)\n",
    "\n",
    "# Support Vector Machine (SVM) Classifier\n",
    "svm_classifier = SVC(probability=True, kernel='linear', C=1.0)\n",
    "\n",
    "# K-Nearest Neighbors (KNN) Classifier\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Voting Classifier\n",
    "voting_classifier = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('adaboost', adaboost_classifier),\n",
    "        ('svm', svm_classifier),\n",
    "        ('knn', knn_classifier)\n",
    "    ],\n",
    "    voting='soft'  # 'hard' for majority voting, 'soft' for weighted voting based on probabilities\n",
    ")\n",
    "\n",
    "# Train the classifiers\n",
    "adaboost_classifier.fit(X_train, y_train)\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "voting_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "adaboost_pred = adaboost_classifier.predict(X_test)\n",
    "svm_pred = svm_classifier.predict(X_test)\n",
    "knn_pred = knn_classifier.predict(X_test)\n",
    "voting_pred = voting_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the performance\n",
    "print(f\"AdaBoost Accuracy: {accuracy_score(y_test, adaboost_pred)}\")\n",
    "print(f\"SVM Accuracy: {accuracy_score(y_test, svm_pred)}\")\n",
    "print(f\"KNN Accuracy: {accuracy_score(y_test, knn_pred)}\")\n",
    "print(f\"Voting Classifier Accuracy: {accuracy_score(y_test, voting_pred)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f25c8914",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.09792284866468842\n",
      "SVM Accuracy: 0.9683481701285855\n",
      "KNN Accuracy: 0.9683481701285855\n",
      "Random Forest Accuracy: 0.9683481701285855\n",
      "Voting Classifier Accuracy: 0.9683481701285855\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Decision Tree Classifier\n",
    "base_classifier_dt = DecisionTreeClassifier(max_depth=5)\n",
    "\n",
    "# Support Vector Machine (SVM) Classifier\n",
    "base_classifier_svm = SVC(probability=True, kernel='linear', C=1.0)\n",
    "\n",
    "# K-Nearest Neighbors (KNN) Classifier\n",
    "base_classifier_knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Random Forest Classifier\n",
    "base_classifier_rf = RandomForestClassifier(n_estimators=150, random_state=42)\n",
    "\n",
    "# Voting Classifier\n",
    "voting_classifier = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('decision_tree', base_classifier_dt),\n",
    "        ('svm', base_classifier_svm),\n",
    "        ('knn', base_classifier_knn),\n",
    "        ('random_forest', base_classifier_rf)\n",
    "    ],\n",
    "    voting='soft'  # 'hard' for majority voting, 'soft' for weighted voting based on probabilities\n",
    ")\n",
    "\n",
    "# Train the classifiers\n",
    "base_classifier_dt.fit(X_train, y_train)\n",
    "base_classifier_svm.fit(X_train, y_train)\n",
    "base_classifier_knn.fit(X_train, y_train)\n",
    "base_classifier_rf.fit(X_train, y_train)\n",
    "voting_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "dt_pred = base_classifier_dt.predict(X_test)\n",
    "svm_pred = base_classifier_svm.predict(X_test)\n",
    "knn_pred = base_classifier_knn.predict(X_test)\n",
    "rf_pred = base_classifier_rf.predict(X_test)\n",
    "voting_pred = voting_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the performance\n",
    "print(f\"Decision Tree Accuracy: {accuracy_score(y_test, dt_pred)}\")\n",
    "print(f\"SVM Accuracy: {accuracy_score(y_test, svm_pred)}\")\n",
    "print(f\"KNN Accuracy: {accuracy_score(y_test, knn_pred)}\")\n",
    "print(f\"Random Forest Accuracy: {accuracy_score(y_test, rf_pred)}\")\n",
    "print(f\"Voting Classifier Accuracy: {accuracy_score(y_test, voting_pred)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b8a5498",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.9683481701285855\n",
      "Gradient Boosting Accuracy: 0.1493570722057369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.09792284866468842\n",
      "SVM Accuracy: 0.9683481701285855\n",
      "KNN Accuracy: 0.9683481701285855\n",
      "Random Forest Accuracy: 0.9683481701285855\n",
      "AdaBoost Accuracy: 0.1671612265084075\n",
      "Voting Classifier Accuracy: 0.9683481701285855\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Decision Tree Classifier\n",
    "base_classifier_dt = DecisionTreeClassifier(max_depth=5)\n",
    "base_classifier_dt.fit(X_train, y_train)\n",
    "dt_pred = base_classifier_dt.predict(X_test)\n",
    "dt_accuracy = accuracy_score(y_test, dt_pred)\n",
    "\n",
    "# Support Vector Machine (SVM) Classifier\n",
    "base_classifier_svm = SVC(probability=True, kernel='linear', C=1.0)\n",
    "base_classifier_svm.fit(X_train, y_train)\n",
    "svm_pred = base_classifier_svm.predict(X_test)\n",
    "svm_accuracy = accuracy_score(y_test, svm_pred)\n",
    "# RandomForest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "rf_pred = rf_classifier.predict(X_test)\n",
    "print(f\"Random Forest Accuracy: {accuracy_score(y_test, rf_pred)}\")\n",
    "\n",
    "# GradientBoosting Classifier\n",
    "gb_classifier = GradientBoostingClassifier(n_estimators=20, learning_rate=1.0, max_depth=1, random_state=42)\n",
    "gb_classifier.fit(X_train, y_train)\n",
    "gb_pred = gb_classifier.predict(X_test)\n",
    "print(f\"Gradient Boosting Accuracy: {accuracy_score(y_test, gb_pred)}\")\n",
    "\n",
    "# SVM Classifier\n",
    "svm_classifier = SVC(probability=True, kernel='linear', C=1.0)\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "svm_pred = svm_classifier.predict(X_test)\n",
    "svm_accuracy = accuracy_score(y_test, svm_pred)\n",
    "\n",
    "# K-Nearest Neighbors (KNN) Classifier\n",
    "base_classifier_knn = KNeighborsClassifier(n_neighbors=5)\n",
    "base_classifier_knn.fit(X_train, y_train)\n",
    "knn_pred = base_classifier_knn.predict(X_test)\n",
    "knn_accuracy = accuracy_score(y_test, knn_pred)\n",
    "\n",
    "# Random Forest Classifier\n",
    "base_classifier_rf = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "base_classifier_rf.fit(X_train, y_train)\n",
    "rf_pred = base_classifier_rf.predict(X_test)\n",
    "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
    "\n",
    "# AdaBoost Classifier\n",
    "base_classifier_ada = AdaBoostClassifier(n_estimators=50, learning_rate=1)\n",
    "base_classifier_ada.fit(X_train, y_train)\n",
    "ada_pred = base_classifier_ada.predict(X_test)\n",
    "ada_accuracy = accuracy_score(y_test, ada_pred)\n",
    "\n",
    "# Voting Classifier\n",
    "voting_classifier = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('decision_tree', base_classifier_dt),\n",
    "        ('svm', base_classifier_svm),\n",
    "        ('knn', base_classifier_knn),\n",
    "        ('random_forest', base_classifier_rf),\n",
    "        ('ada_boost', base_classifier_ada)\n",
    "    ],\n",
    "    voting='soft'  # 'hard' for majority voting, 'soft' for weighted voting based on probabilities\n",
    ")\n",
    "\n",
    "voting_classifier.fit(X_train, y_train)\n",
    "voting_pred = voting_classifier.predict(X_test)\n",
    "voting_accuracy = accuracy_score(y_test, voting_pred)\n",
    "\n",
    "# Print accuracies together\n",
    "print(f\"Decision Tree Accuracy: {dt_accuracy}\")\n",
    "print(f\"SVM Accuracy: {svm_accuracy}\")\n",
    "print(f\"KNN Accuracy: {knn_accuracy}\")\n",
    "print(f\"Random Forest Accuracy: {rf_accuracy}\")\n",
    "print(f\"AdaBoost Accuracy: {ada_accuracy}\")\n",
    "print(f\"Voting Classifier Accuracy: {voting_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e49ee86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
