{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41f27d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.83\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Loading the dataset\n",
    "df = pd.read_csv('output.csv')\n",
    "\n",
    "# Split the dataset into features (X) and target (y)\n",
    "X = df.drop('Disease', axis=1)\n",
    "y = df['Disease']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the RandomForestClassifier with multiple Decision Trees (n_estimators)\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# checking which diseases are predicted wrong \n",
    "# for actual, predicted in zip(y_test, y_pred):\n",
    "#     if actual != predicted:\n",
    "#         print(f\"Actual: {actual}, Predicted: {predicted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83261d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.83\n"
     ]
    }
   ],
   "source": [
    "# Initialize the RandomForestClassifier with multiple Decision Trees (n_estimators)\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "print(f'Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abee8e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the 'clf' model to a pickle file\n",
    "with open('model.pkl', 'wb') as file:\n",
    "    pickle.dump(clf, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c598075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in y: {'Diverticulosis', 'Thrombocytopaenia', 'Hypercholesterolemia', 'Asthma', 'Myocardial Infarction', 'Neoplasm Metastasis', 'Dehydration', 'Paroxysmaldyspnea', 'Hypertension ', 'Bronchitis', 'Ischemia', 'Gastroesophageal Reflux Disease', 'Osteomyelitis', 'Sickle Cell Anemia', 'Obesity Morbid', 'Neoplasm', 'Chronic Alcoholic Intoxication', 'Primary Malignant Neoplasm', 'Schizophrenia', 'Varicose veins', 'Pancreatitis', 'Anemia', 'Tuberculosis', 'Encephalopathy', 'Psychotic Disorder', 'Paranoia', 'Epilepsy', 'Arthritis', 'Diabetes', 'Paralysis (brain hemorrhage)', 'Pneumonia', 'Cardiomyopathy', 'Herniahiatal', 'Alcoholic hepatitis', 'Cirrhosis', 'Cholecystitis', 'Kidney Failure Acute', 'Adhesion', 'Hepatitis B', 'Delusion', 'Bronchial Asthma', 'Dengue', 'Impetigo', 'Hiv Infections', 'Failure Heart Congestive', 'Sepsis (Invertebrate)', 'Colitis', 'Peripheral Vascular Disease', 'Hepatitis E', 'Pneumothorax', 'Migraine', 'Failure Kidney', 'Hypothyroidism', 'Common Cold', 'Infection', 'Pneumocystiscariniipneumonia', 'Dimorphic hemmorhoids(piles)', 'Hyperthyroidism', 'Ulcer Peptic', 'Gout', 'Transient Ischemic Attack', 'Insufficiency Renal', 'Jaundice', 'Influenza', 'Hepatitis', 'Failure Heart', 'Ketoacidosis Diabetic', 'Chronic Obstructive Airway Disease', 'Coronary Heart Disease', 'Obesity', 'Affect Labile', 'Hyperglycemia', 'Fibroid Tumor', 'Diabetes ', 'Carcinoma Breast', 'Hyperlipidemia', 'Fungal infection', 'Primary Carcinoma Of The Liver Cells', 'Lymphoma', 'Upper Respiratory Infection', 'Decubitus Ulcer', 'Embolism Pulmonary', 'Spasm Bronchial', 'Cervical spondylosis', 'Hepatitis D', 'Ileus', 'Heart attack', 'Pyelonephritis', 'Dependence', 'Chronic cholestasis', 'Gastritis', 'Bacteremia', 'Glaucoma', 'Dementia', 'Allergy', 'AIDS', 'Malignantneoplasms', 'Psoriasis', 'Carcinoma Colon', 'Osteoarthristis', 'Melanoma', 'Lymphatic Diseases', 'Neuropathy', 'Adenocarcinoma', \"Alzheimer'S Disease\", 'Benign Prostatic Hypertrophy', 'Exanthema', 'Hypoglycemia', 'Pneumonia Aspiration', 'Carcinoma', 'hepatitis A', 'Thrombus', 'Peptic ulcer diseae', 'Anxiety State', 'Overload Fluid', 'Tonic-Clonic Seizures', 'Aphasia', 'Kidney Disease', 'Acne', 'Depressive Disorder', 'Infection Urinary Tract', 'Personality Disorder', 'Bipolar Disorder', 'Pericardial Effusion Body Substance', 'Hyperbilirubinemia', 'Carcinoma Prostate', 'Malaria', 'Parkinson Disease', 'Tachycardia Sinus', 'Migraine Disorders', 'Degenerativepolyarthritis', 'Deep Vein Thrombosis', 'Oralcandidiasis', 'Typhoid', 'Drug Reaction', 'Biliary Calculus', 'Hemiparesis', 'Gastroenteritis', 'Urinary tract infection', 'Hepatitis C', 'Edema Pulmonary', 'Carcinoma Of Lung', 'Suicide Attempt', 'Chronic Kidney Failure', 'Neutropenia', 'Cellulitis', 'Delirium', 'Hypertension Pulmonary', 'Hypertensive Disease', 'GERD', 'Pancytopenia', 'Incontinence', 'Confusion', 'Respiratory Failure', 'Osteoporosis', 'Stenosis Aortic Valve', '(vertigo) Paroymsal  Positional Vertigo', 'Tricuspid Valve Insufficiency', 'Diverticulitis', 'Accidentcerebrovascular', 'Chicken pox', 'Manic Disorder', 'Deglutition Disorder', 'Hemorrhoids', 'Mitral Valve Insufficiency', 'Endocarditis', 'Emphysema Pulmonary', 'Hernia'}\n",
      "Unique values in y_encoded: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Assuming your target variable is 'y'\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Print unique values before and after encoding\n",
    "print(\"Unique values in y:\", set(y))\n",
    "print(\"Unique values in y_encoded:\", set(y_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7a7f763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 96.83\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Create an SVM classifier\n",
    "model = make_pipeline(StandardScaler(), SVC())\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "print(f\"SVM Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c0356e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy: 96.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Create a KNN classifier\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "print(f\"KNN Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5056d332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 96.83\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Create a Naive Bayes classifier\n",
    "model = GaussianNB()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "print(f\"Naive Bayes Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00e12c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy: 0.1671612265084075\n",
      "SVM Accuracy: 0.9683481701285855\n",
      "KNN Accuracy: 0.9683481701285855\n",
      "Voting Classifier Accuracy: 0.9683481701285855\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# AdaBoost Classifier\n",
    "base_classifier = DecisionTreeClassifier(max_depth=1)  # You can use any classifier as the base learner\n",
    "adaboost_classifier = AdaBoostClassifier(base_classifier, n_estimators=50, learning_rate=1.0, random_state=42)\n",
    "\n",
    "# Support Vector Machine (SVM) Classifier\n",
    "svm_classifier = SVC(probability=True, kernel='linear', C=1.0)\n",
    "\n",
    "# K-Nearest Neighbors (KNN) Classifier\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Voting Classifier\n",
    "voting_classifier = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('adaboost', adaboost_classifier),\n",
    "        ('svm', svm_classifier),\n",
    "        ('knn', knn_classifier)\n",
    "    ],\n",
    "    voting='soft'  # 'hard' for majority voting, 'soft' for weighted voting based on probabilities\n",
    ")\n",
    "\n",
    "# Train the classifiers\n",
    "adaboost_classifier.fit(X_train, y_train)\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "voting_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "adaboost_pred = adaboost_classifier.predict(X_test)\n",
    "svm_pred = svm_classifier.predict(X_test)\n",
    "knn_pred = knn_classifier.predict(X_test)\n",
    "voting_pred = voting_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the performance\n",
    "print(f\"AdaBoost Accuracy: {accuracy_score(y_test, adaboost_pred)}\")\n",
    "print(f\"SVM Accuracy: {accuracy_score(y_test, svm_pred)}\")\n",
    "print(f\"KNN Accuracy: {accuracy_score(y_test, knn_pred)}\")\n",
    "print(f\"Voting Classifier Accuracy: {accuracy_score(y_test, voting_pred)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f25c8914",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.09792284866468842\n",
      "SVM Accuracy: 0.9683481701285855\n",
      "KNN Accuracy: 0.9683481701285855\n",
      "Random Forest Accuracy: 0.9683481701285855\n",
      "Voting Classifier Accuracy: 0.9683481701285855\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Decision Tree Classifier\n",
    "base_classifier_dt = DecisionTreeClassifier(max_depth=5)\n",
    "\n",
    "# Support Vector Machine (SVM) Classifier\n",
    "base_classifier_svm = SVC(probability=True, kernel='linear', C=1.0)\n",
    "\n",
    "# K-Nearest Neighbors (KNN) Classifier\n",
    "base_classifier_knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Random Forest Classifier\n",
    "base_classifier_rf = RandomForestClassifier(n_estimators=150, random_state=42)\n",
    "\n",
    "# Voting Classifier\n",
    "voting_classifier = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('decision_tree', base_classifier_dt),\n",
    "        ('svm', base_classifier_svm),\n",
    "        ('knn', base_classifier_knn),\n",
    "        ('random_forest', base_classifier_rf)\n",
    "    ],\n",
    "    voting='soft'  # 'hard' for majority voting, 'soft' for weighted voting based on probabilities\n",
    ")\n",
    "\n",
    "# Train the classifiers\n",
    "base_classifier_dt.fit(X_train, y_train)\n",
    "base_classifier_svm.fit(X_train, y_train)\n",
    "base_classifier_knn.fit(X_train, y_train)\n",
    "base_classifier_rf.fit(X_train, y_train)\n",
    "voting_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "dt_pred = base_classifier_dt.predict(X_test)\n",
    "svm_pred = base_classifier_svm.predict(X_test)\n",
    "knn_pred = base_classifier_knn.predict(X_test)\n",
    "rf_pred = base_classifier_rf.predict(X_test)\n",
    "voting_pred = voting_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the performance\n",
    "print(f\"Decision Tree Accuracy: {accuracy_score(y_test, dt_pred)}\")\n",
    "print(f\"SVM Accuracy: {accuracy_score(y_test, svm_pred)}\")\n",
    "print(f\"KNN Accuracy: {accuracy_score(y_test, knn_pred)}\")\n",
    "print(f\"Random Forest Accuracy: {accuracy_score(y_test, rf_pred)}\")\n",
    "print(f\"Voting Classifier Accuracy: {accuracy_score(y_test, voting_pred)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b8a5498",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.9683481701285855\n",
      "Gradient Boosting Accuracy: 0.1493570722057369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.09792284866468842\n",
      "SVM Accuracy: 0.9683481701285855\n",
      "KNN Accuracy: 0.9683481701285855\n",
      "Random Forest Accuracy: 0.9683481701285855\n",
      "AdaBoost Accuracy: 0.1671612265084075\n",
      "Voting Classifier Accuracy: 0.9683481701285855\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Decision Tree Classifier\n",
    "base_classifier_dt = DecisionTreeClassifier(max_depth=5)\n",
    "base_classifier_dt.fit(X_train, y_train)\n",
    "dt_pred = base_classifier_dt.predict(X_test)\n",
    "dt_accuracy = accuracy_score(y_test, dt_pred)\n",
    "\n",
    "# Support Vector Machine (SVM) Classifier\n",
    "base_classifier_svm = SVC(probability=True, kernel='linear', C=1.0)\n",
    "base_classifier_svm.fit(X_train, y_train)\n",
    "svm_pred = base_classifier_svm.predict(X_test)\n",
    "svm_accuracy = accuracy_score(y_test, svm_pred)\n",
    "# RandomForest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "rf_pred = rf_classifier.predict(X_test)\n",
    "print(f\"Random Forest Accuracy: {accuracy_score(y_test, rf_pred)}\")\n",
    "\n",
    "# GradientBoosting Classifier\n",
    "gb_classifier = GradientBoostingClassifier(n_estimators=20, learning_rate=1.0, max_depth=1, random_state=42)\n",
    "gb_classifier.fit(X_train, y_train)\n",
    "gb_pred = gb_classifier.predict(X_test)\n",
    "print(f\"Gradient Boosting Accuracy: {accuracy_score(y_test, gb_pred)}\")\n",
    "\n",
    "# SVM Classifier\n",
    "svm_classifier = SVC(probability=True, kernel='linear', C=1.0)\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "svm_pred = svm_classifier.predict(X_test)\n",
    "svm_accuracy = accuracy_score(y_test, svm_pred)\n",
    "\n",
    "# K-Nearest Neighbors (KNN) Classifier\n",
    "base_classifier_knn = KNeighborsClassifier(n_neighbors=5)\n",
    "base_classifier_knn.fit(X_train, y_train)\n",
    "knn_pred = base_classifier_knn.predict(X_test)\n",
    "knn_accuracy = accuracy_score(y_test, knn_pred)\n",
    "\n",
    "# Random Forest Classifier\n",
    "base_classifier_rf = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "base_classifier_rf.fit(X_train, y_train)\n",
    "rf_pred = base_classifier_rf.predict(X_test)\n",
    "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
    "\n",
    "# AdaBoost Classifier\n",
    "base_classifier_ada = AdaBoostClassifier(n_estimators=50, learning_rate=1)\n",
    "base_classifier_ada.fit(X_train, y_train)\n",
    "ada_pred = base_classifier_ada.predict(X_test)\n",
    "ada_accuracy = accuracy_score(y_test, ada_pred)\n",
    "\n",
    "# Voting Classifier\n",
    "voting_classifier = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('decision_tree', base_classifier_dt),\n",
    "        ('svm', base_classifier_svm),\n",
    "        ('knn', base_classifier_knn),\n",
    "        ('random_forest', base_classifier_rf),\n",
    "        ('ada_boost', base_classifier_ada)\n",
    "    ],\n",
    "    voting='soft'  # 'hard' for majority voting, 'soft' for weighted voting based on probabilities\n",
    ")\n",
    "\n",
    "voting_classifier.fit(X_train, y_train)\n",
    "voting_pred = voting_classifier.predict(X_test)\n",
    "voting_accuracy = accuracy_score(y_test, voting_pred)\n",
    "\n",
    "# Print accuracies together\n",
    "print(f\"Decision Tree Accuracy: {dt_accuracy}\")\n",
    "print(f\"SVM Accuracy: {svm_accuracy}\")\n",
    "print(f\"KNN Accuracy: {knn_accuracy}\")\n",
    "print(f\"Random Forest Accuracy: {rf_accuracy}\")\n",
    "print(f\"AdaBoost Accuracy: {ada_accuracy}\")\n",
    "print(f\"Voting Classifier Accuracy: {voting_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e49ee86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier Accuracy: 0.9683481701285855\n",
      "Voting Classifier Precision: 0.9516013034728258\n",
      "Voting Classifier Recall: 0.9683481701285855\n",
      "Voting Classifier F1 Score: 0.9583380624599073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "G:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "voting_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Voting Classifier Accuracy: {voting_accuracy}\")\n",
    "\n",
    "voting_precision = precision_score(y_test, y_pred, average='weighted')\n",
    "voting_recall = recall_score(y_test, y_pred, average='weighted')\n",
    "voting_f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Voting Classifier Precision: {voting_precision}\")\n",
    "print(f\"Voting Classifier Recall: {voting_recall}\")\n",
    "print(f\"Voting Classifier F1 Score: {voting_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ee17add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.54\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "df = pd.read_csv('output.csv')\n",
    "\n",
    "# Split the dataset into features (X) and target (y)\n",
    "X = df.drop('Disease', axis=1)\n",
    "y = df['Disease']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Initialize the RandomForestClassifier with multiple Decision Trees (n_estimators)\n",
    "clf =DecisionTreeClassifier()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "print(f'Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8decdd60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
